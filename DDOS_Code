#Kafka Producer Using Python

import json
from pyspark.sql import types as spark_type
from pyspark.sql import SparkSession # this will help me run this code as a program

schema_string = """
{"fields":[
        {"metadata":{},"name":"ip","nullable":true,"type":"string"},
        {"metadata":{},"name":"_c1","nullable":true,"type":"string"},
        {"metadata":{},"name":"_c2","nullable":true,"type":"string"},
        {"metadata":{},"name":"Time","nullable":true,"type":"string"},
        {"metadata":{},"name":"Time_Zone","nullable":true,"type":"string"},
        {"metadata":{},"name":"Request","nullable":true,"type":"string"},
        {"metadata":{},"name":"Status","nullable":true,"type":"string"},
        {"metadata":{},"name":"Size","nullable":true,"type":"string"},
        {"metadata":{},"name":"Referer","nullable":true,"type":"string"},
        {"metadata":{},"name":"User_Agent","nullable":true,"type":"string"}
],"type":"struct"}
"""
schema = spark_type.StructType.fromJson(json.loads(schema_string))

spark = SparkSession.builder.appName("phdata_producer").getOrCreate() # initalizing spark

df = spark.read.csv("/phdata/apache-access-log.txt",sep=" ", schema=schema)\
    .select("ip","Time","Time_Zone","Request","Status","Size","Referer","User_Agent")

def generate_json_payload_from_columns(row):
        payload = {}
        for f in row.__fields__:
            payload[f] = row[f]
        return payload

df_json = spark.createDataFrame(df.rdd.map(lambda x: {"value": json.dumps(generate_json_payload_from_columns(x))}))

df_json.write.format("kafka") \
    .option("kafka.bootstrap.servers", "sandbox-hdp.hortonworks.com:9092") \
    .option("topic", "phdata9") \
    .save()
